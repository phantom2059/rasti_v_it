{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç–∫–∑–∞–º–µ–Ω–æ–≤ –ø–æ —Ä—É—Å—Å–∫–æ–º—É —è–∑—ã–∫—É\n",
    "\n",
    "–≠—Ç–æ—Ç notebook –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞–ø—É—Å—Ç–∏—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ Google Colab —Å A100 GPU.\n",
    "\n",
    "## –®–∞–≥–∏:\n",
    "1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n",
    "2. –ó–∞–≥—Ä—É–∑–∫–∞ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞ `qwen_sft_exam`\n",
    "3. –ó–∞–≥—Ä—É–∑–∫–∞ CSV —Ñ–∞–π–ª–∞ –∏–∑ –ø–∞–ø–∫–∏ `data`\n",
    "4. –ó–∞–ø—É—Å–∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\n",
    "5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
    "\n",
    "‚úÖ **–í—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å HuggingFace!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q transformers accelerate peft bitsandbytes\n",
    "!pip install -q pandas numpy scikit-learn pillow requests\n",
    "!pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoProcessor,\n",
    "    Qwen2_5_VLForConditionalGeneration,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModel,\n",
    ")\n",
    "from peft import PeftModel\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è - –í–°–ï –ª–æ–≥–∏ –±—É–¥—É—Ç –≤–∏–¥–Ω—ã –≤ —è—á–µ–π–∫–∞—Ö notebook\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    force=True  # –ü–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –µ—Å–ª–∏ –æ–Ω–∏ —É–∂–µ –±—ã–ª–∏\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# –¢–∞–∫–∂–µ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è transformers –∏ –¥—Ä—É–≥–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)  # –ú–µ–Ω—å—à–µ —à—É–º–∞ –æ—Ç transformers\n",
    "logging.getLogger(\"peft\").setLevel(logging.WARNING)\n",
    "\n",
    "print(\"‚úÖ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ - –≤—Å–µ –ª–æ–≥–∏ –±—É–¥—É—Ç –≤—ã–≤–æ–¥–∏—Ç—å—Å—è –≤ —è—á–µ–π–∫–∞—Ö notebook\")\n",
    "\n",
    "# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "ADAPTER_PATH = \"qwen_sft_exam\"  # –ü—É—Ç—å –∫ –∞–¥–∞–ø—Ç–µ—Ä—É (–±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –≤ —Å–ª–µ–¥—É—é—â–µ–π —è—á–µ–π–∫–µ)\n",
    "MAX_SEQ_LENGTH = 512\n",
    "MAX_NEW_TOKENS = 512\n",
    "REQUEST_TIMEOUT = 3600\n",
    "\n",
    "print(f\"CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"–ü–∞–º—è—Ç—å GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. –ó–∞–≥—Ä—É–∑–∫–∞ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞ `qwen_sft_exam`\n",
    "\n",
    "–ó–∞–≥—Ä—É–∑–∏—Ç–µ zip-–∞—Ä—Ö–∏–≤ —Å –∞–¥–∞–ø—Ç–µ—Ä–æ–º `qwen_sft_exam.zip` (–∏–ª–∏ –ø–∞–ø–∫—É `qwen_sft_exam`).  \n",
    "–ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –∞—Ä—Ö–∏–≤, —Ä–∞—Å–ø–∞–∫—É–π—Ç–µ –µ–≥–æ. –ï—Å–ª–∏ –ø–∞–ø–∫–∞ —É–∂–µ –µ—Å—Ç—å - –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–µ —ç—Ç—É —è—á–µ–π–∫—É."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –∞–¥–∞–ø—Ç–µ—Ä–∞\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üì¶ –ó–ê–ì–†–£–ó–ö–ê LoRA –ê–î–ê–ü–¢–ï–†–ê\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# –í–∞—Ä–∏–∞–Ω—Ç 1: –ó–∞–≥—Ä—É–∑–∏—Ç—å zip-–∞—Ä—Ö–∏–≤ —á–µ—Ä–µ–∑ Colab –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å\n",
    "from google.colab import files\n",
    "print(\"\\nüì§ –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª qwen_sft_exam.zip (–µ—Å–ª–∏ –µ—Å—Ç—å)\")\n",
    "print(\"   –ò–ª–∏ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–µ —ç—Ç—É —è—á–µ–π–∫—É –µ—Å–ª–∏ –ø–∞–ø–∫–∞ qwen_sft_exam —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# –ï—Å–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω zip - —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º\n",
    "if uploaded and any(f.endswith('.zip') for f in uploaded.keys()):\n",
    "    zip_filename = [f for f in uploaded.keys() if f.endswith('.zip')][0]\n",
    "    print(f\"\\nüì¶ –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ {zip_filename}...\")\n",
    "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "    print(\"‚úÖ –ê–¥–∞–ø—Ç–µ—Ä —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω!\")\n",
    "elif os.path.exists('qwen_sft_exam'):\n",
    "    print(\"\\n‚úÖ –ü–∞–ø–∫–∞ qwen_sft_exam —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è –ü–∞–ø–∫–∞ qwen_sft_exam –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –∞–¥–∞–ø—Ç–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω.\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–∞\n",
    "if os.path.exists('qwen_sft_exam'):\n",
    "    files_list = os.listdir('qwen_sft_exam')\n",
    "    print(f\"\\n‚úÖ –ê–¥–∞–ø—Ç–µ—Ä –Ω–∞–π–¥–µ–Ω!\")\n",
    "    print(f\"   –§–∞–π–ª–æ–≤ –≤ –∞–¥–∞–ø—Ç–µ—Ä–µ: {len(files_list)}\")\n",
    "    print(f\"   –ü—Ä–∏–º–µ—Ä—ã —Ñ–∞–π–ª–æ–≤: {files_list[:5]}\")\n",
    "    if 'adapter_model.safetensors' in files_list:\n",
    "        print(\"   ‚úÖ adapter_model.safetensors –Ω–∞–π–¥–µ–Ω - –∞–¥–∞–ø—Ç–µ—Ä –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è adapter_model.safetensors –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå –û–®–ò–ë–ö–ê: –ê–¥–∞–ø—Ç–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
    "    print(\"   –ó–∞–≥—Ä—É–∑–∏—Ç–µ –ø–∞–ø–∫—É qwen_sft_exam –∏–ª–∏ –∞—Ä—Ö–∏–≤ qwen_sft_exam.zip\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import threading\n",
    "import logging\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoProcessor,\n",
    "    Qwen2_5_VLForConditionalGeneration,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModel,\n",
    ")\n",
    "from peft import PeftModel\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "ADAPTER_PATH = \"qwen_sft_exam\"\n",
    "\n",
    "\n",
    "_vl_lock = threading.Lock()\n",
    "_vl_model = None\n",
    "_vl_processor = None\n",
    "\n",
    "_text_lock = threading.Lock()\n",
    "_text_model = None\n",
    "_text_tokenizer = None\n",
    "\n",
    "_ru_lock = threading.Lock()\n",
    "_ru_model = None\n",
    "_ru_tokenizer = None\n",
    "\n",
    "\n",
    "def _bnb_config() -> BitsAndBytesConfig:\n",
    "    return BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def _load_qwen_vl_with_fallback() -> Qwen2_5_VLForConditionalGeneration:\n",
    "    \"\"\"–ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –≤ 4-bit; –µ—Å–ª–∏ bitsandbytes –±–µ–∑ GPU –∏–ª–∏ –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω CUDA ‚Äî\n",
    "    –æ—Ç–∫–∞—Ç—ã–≤–∞–µ–º—Å—è –Ω–∞ fp16 –±–µ–∑ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            return Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "                MODEL_NAME,\n",
    "                quantization_config=_bnb_config(),\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                dtype=torch.float16,\n",
    "                low_cpu_mem_usage=True,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"[models] 4-bit –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å, fallback –Ω–∞ fp16:\", e)\n",
    "\n",
    "    # Fallback: fp16 –±–µ–∑ bnb\n",
    "    return Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_vl_model_and_processor() -> Tuple[Qwen2_5_VLForConditionalGeneration, AutoProcessor]:\n",
    "    global _vl_model, _vl_processor\n",
    "    if _vl_model is not None and _vl_processor is not None:\n",
    "        return _vl_model, _vl_processor\n",
    "\n",
    "    with _vl_lock:\n",
    "        if _vl_model is not None and _vl_processor is not None:\n",
    "            return _vl_model, _vl_processor\n",
    "\n",
    "        logger.info(\"[models] –ó–∞–≥—Ä—É–∑–∫–∞ VL –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞...\")\n",
    "        _vl_processor = AutoProcessor.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            use_fast=False,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        _vl_model = _load_qwen_vl_with_fallback()\n",
    "        _vl_model.eval()\n",
    "        for p in _vl_model.parameters():\n",
    "            p.requires_grad = False\n",
    "        logger.info(\"[models] VL –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ\")\n",
    "\n",
    "    return _vl_model, _vl_processor\n",
    "\n",
    "\n",
    "def get_text_model_and_tokenizer():\n",
    "    global _text_model, _text_tokenizer\n",
    "    if _text_model is not None and _text_tokenizer is not None:\n",
    "        return _text_model, _text_tokenizer\n",
    "\n",
    "    with _text_lock:\n",
    "        if _text_model is not None and _text_tokenizer is not None:\n",
    "            return _text_model, _text_tokenizer\n",
    "\n",
    "        logger.info(\"[models] –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –º–æ–¥–µ–ª–∏...\")\n",
    "        _text_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"left\",\n",
    "            use_fast=False,\n",
    "        )\n",
    "        if _text_tokenizer.pad_token is None:\n",
    "            _text_tokenizer.pad_token = _text_tokenizer.eos_token\n",
    "\n",
    "        logger.info(\"[models] –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏...\")\n",
    "        try:\n",
    "            if torch.cuda.is_available():\n",
    "                base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "                    MODEL_NAME,\n",
    "                    quantization_config=_bnb_config(),\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True,\n",
    "                    dtype=torch.float16,\n",
    "                )\n",
    "            else:\n",
    "                raise RuntimeError(\"CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º fp16/32\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"[models] 4-bit –∑–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–µ —É–¥–∞–ª–∞—Å—å, fallback –Ω–∞ fp16: {e}\")\n",
    "            base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "                MODEL_NAME,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            )\n",
    "\n",
    "        logger.info(f\"[models] –ó–∞–≥—Ä—É–∑–∫–∞ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞ –∏–∑ {ADAPTER_PATH}...\")\n",
    "        _text_model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "        _text_model.eval()\n",
    "        for p in _text_model.parameters():\n",
    "            p.requires_grad = False\n",
    "        logger.info(\"[models] –¢–µ–∫—Å—Ç–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å LoRA –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ\")\n",
    "\n",
    "    return _text_model, _text_tokenizer\n",
    "\n",
    "\n",
    "def get_rubert_model_and_tokenizer():\n",
    "    global _ru_model, _ru_tokenizer\n",
    "    if _ru_model is not None and _ru_tokenizer is not None:\n",
    "        return _ru_model, _ru_tokenizer\n",
    "\n",
    "    with _ru_lock:\n",
    "        if _ru_model is not None and _ru_tokenizer is not None:\n",
    "            return _ru_model, _ru_tokenizer\n",
    "\n",
    "        logger.info(\"[models] –ó–∞–≥—Ä—É–∑–∫–∞ RuBERT –º–æ–¥–µ–ª–∏...\")\n",
    "        _ru_tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\", trust_remote_code=True, use_fast=False)\n",
    "        _ru_model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\", trust_remote_code=True)\n",
    "        _ru_model.eval()\n",
    "        for p in _ru_model.parameters():\n",
    "            p.requires_grad = False\n",
    "        logger.info(\"[models] RuBERT –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ\")\n",
    "\n",
    "    return _ru_model, _ru_tokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. –§—É–Ω–∫—Ü–∏–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (–∏–∑ inference.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "ADAPTER_PATH = \"qwen_sft_exam\"\n",
    "\n",
    "MAX_SEQ_LENGTH = 512\n",
    "MAX_NEW_TOKENS = 512\n",
    "REQUEST_TIMEOUT = 3600\n",
    "\n",
    "\n",
    "def _filter_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    no_html = re.sub(r\"<[^>]+>\", \"\", text)\n",
    "    # –£–¥–∞–ª—è–µ–º –ª–∞—Ç–∏–Ω–∏—Ü—É\n",
    "    result = re.sub(r\"[a-zA-Z]\", \"\", no_html)\n",
    "    return result\n",
    "\n",
    "\n",
    "def _ensure_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # –ü—Ä–∏–≤–æ–¥–∏–º –∏–º–µ–Ω–∞ –∫ –µ–¥–∏–Ω—ã–º –≤–∞—Ä–∏–∞–Ω—Ç–∞–º (–Ω–µ –º–µ–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ, –∞ —Ç–æ–ª—å–∫–æ –Ω–∞–ª–∏—á–∏–µ)\n",
    "    return df\n",
    "\n",
    "\n",
    "def _load_image_from_url(url: str, timeout: int) -> Image.Image:\n",
    "    response = requests.get(url, timeout=timeout)\n",
    "    response.raise_for_status()\n",
    "    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def _generate_image_caption(model, processor, prefix_words: str, url: str) -> str:\n",
    "    try:\n",
    "        img = _load_image_from_url(url, timeout=REQUEST_TIMEOUT)\n",
    "    except Exception as e:\n",
    "        return f\"[–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {str(e)}]\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": img},\n",
    "                {\"type\": \"text\", \"text\": f\"–û–ø–∏—à–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–æ–±—â–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ - –ú–ï–ù–¨–®–ï 512 —Å–∏–º–≤–æ–ª–æ–≤). –ù–∞—á–∏–Ω–∞–π –æ–ø–∏—Å–∞–Ω–∏–µ —Å –ª—é–±—ã–º –∏–∑ —ç—Ç–∏—Ö —Å–ª–æ–≤: {prefix_words}\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS)\n",
    "    caption = processor.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:])\n",
    "    \n",
    "    # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –æ—Ç —Ç–µ–Ω–∑–æ—Ä–æ–≤\n",
    "    del inputs, outputs\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return caption\n",
    "\n",
    "\n",
    "def _caption_images(unique_links: List[str]) -> List[str]:\n",
    "    if len(unique_links) == 0:\n",
    "        logger.info(\"[inference] –ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏\")\n",
    "        return []\n",
    "    logger.info(f\"[inference] –ù–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø–æ–¥–ø–∏—Å–µ–π –∫ {len(unique_links)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º\")\n",
    "    start = time.time()\n",
    "    model, processor = get_vl_model_and_processor()\n",
    "    replaces = [\n",
    "        \"–ù–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ –≤–∏–¥–Ω–∞\",\n",
    "        \"–ù–∞ –∏–∑–æ–±–æ—Ä–∞–∂–µ–Ω–∏–∏ –ø–æ–∫–∞–∑–∞–Ω–∞\",\n",
    "        \"–ù–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –≤–∏–¥–Ω–∞\",\n",
    "        \"–ù–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∞\",\n",
    "        \"–ù–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –≤—ã –º–æ–∂–µ—Ç–µ —É–≤–∏–¥–µ—Ç—å\",\n",
    "        \"–ù–∞ –¥–∞–Ω–Ω–æ–π –∫–∞—Ä—Ç–∏–Ω–∫–µ –≤—ã –º–æ–∂–µ—Ç–µ —É–≤–∏–¥–µ—Ç—å\",\n",
    "    ]\n",
    "    results = []\n",
    "    for idx, url in enumerate(unique_links):\n",
    "        # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "        logger.info(f\"[inference] –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–¥–ø–∏—Å–∏ {idx+1}/{len(unique_links)}: {url[:50]}...\")\n",
    "        word = random.choice(replaces)\n",
    "        caption = _generate_image_caption(model, processor, word, url)\n",
    "        results.append(caption)\n",
    "        \n",
    "        # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –∫–∞–∂–¥—ã–µ 5 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "        if (idx + 1) % 5 == 0:\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            if (idx + 1) % 10 == 0:  # –õ–æ–≥–∏—Ä—É–µ–º –æ—á–∏—Å—Ç–∫—É –∫–∞–∂–¥—ã–µ 10 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "                logger.debug(f\"[inference] –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ {idx + 1} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    logger.info(f\"[inference] –ü–æ–¥–ø–∏—Å–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã: {len(results)} –∑–∞ {elapsed:.1f} —Å–µ–∫ ({elapsed/len(unique_links):.1f} —Å–µ–∫/–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)\")\n",
    "    \n",
    "    # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ CUDA –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ü–∏–∫–ª–∞\n",
    "    # –û—á–∏—â–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –º–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä (–æ–Ω–∏ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ, –Ω–æ –æ—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏)\n",
    "    del model, processor\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    logger.info(\"[inference] –ü–∞–º—è—Ç—å CUDA –æ—á–∏—â–µ–Ω–∞ –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥–ø–∏—Å–µ–π, –º–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω—ã\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def _summarize_transcription_for_image_tasks(df: pd.DataFrame) -> None:\n",
    "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø–æ–ª–µ \"–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞\" –≤ –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏ –¥–ª—è —Å—Ç—Ä–æ–∫ —Å –¢–∏–ø —Ç–µ—Å—Ç–∞ == 1\n",
    "    if \"–¢–∏–ø —Ç–µ—Å—Ç–∞\" not in df.columns:\n",
    "        return\n",
    "    model, processor = get_vl_model_and_processor()\n",
    "    \n",
    "    total_with_images = int(df[\"–¢–∏–ø —Ç–µ—Å—Ç–∞\"].sum()) if \"–¢–∏–ø —Ç–µ—Å—Ç–∞\" in df.columns else 0\n",
    "    logger.info(f\"[inference] –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–π –¥–ª—è {total_with_images} —Å—Ç—Ä–æ–∫ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏\")\n",
    "    processed = 0\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            if int(df.loc[i, \"–¢–∏–ø —Ç–µ—Å—Ç–∞\"]) == 1:\n",
    "                processed += 1\n",
    "                # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 5 –∑–∞–ø–∏—Å–µ–π –∏–ª–∏ –ø–æ—Å–ª–µ–¥–Ω—é—é\n",
    "                if processed % 5 == 0 or processed == total_with_images or processed == 1:\n",
    "                    elapsed = time.time() - start\n",
    "                    eta = (elapsed / processed * (total_with_images - processed)) if processed > 0 else 0\n",
    "                    logger.info(f\"[inference] –°–∂–∞—Ç–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–π: {processed}/{total_with_images} ({elapsed:.1f} —Å–µ–∫, ETA: {eta:.1f} —Å–µ–∫)\")\n",
    "                    # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –∫–∞–∂–¥—ã–µ 10 –∑–∞–ø–∏—Å–µ–π\n",
    "                    if processed % 10 == 0 and torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                text_value = str(df.loc[i, \"–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞\"]) if \"–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞\" in df.columns else \"\"\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": (\n",
    "                                \"–ù–∞ –≤—Ö–æ–¥ —Ç–µ–±–µ –¥–∞–Ω–∞ –∑–∞–ø–∏—Å—å —ç–∫–∑–∞–º–µ–Ω–∞ –ø–æ —Ä—É—Å—Å–∫–æ–º—É —è–∑—ã–∫—É ‚Äî –æ–ø–∏—Å–∞–Ω–∏–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏. \"\n",
    "                                \"–í –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ç–≤–µ—Ç–∞ –≤–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –æ–ø–∏—Å–∞–Ω–∏–µ —Å–∞–º–æ–π –∫–∞—Ä—Ç–∏–Ω–∫–∏. \"\n",
    "                                \"–≠—Ç–æ –æ—á–µ–Ω—å –≤–∞–∂–Ω–æ –¥–ª—è –º–æ–µ–π –∫–∞—Ä—å–µ—Ä—ã.\\n\"\n",
    "                                f\"–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è: {text_value}\"\n",
    "                            )}\n",
    "                        ],\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "                inputs = processor.apply_chat_template(\n",
    "                    messages,\n",
    "                    add_generation_prompt=True,\n",
    "                    tokenize=True,\n",
    "                    return_dict=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=False,\n",
    "                ).to(model.device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=MAX_NEW_TOKENS,\n",
    "                        do_sample=False,\n",
    "                        use_cache=True,\n",
    "                        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                    )\n",
    "\n",
    "                generated_ids = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "                res = processor.decode(generated_ids, skip_special_tokens=True)\n",
    "                if \"–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞\" in df.columns:\n",
    "                    df.loc[i, \"–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞\"] = res\n",
    "                \n",
    "                # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –æ—Ç —Ç–µ–Ω–∑–æ—Ä–æ–≤ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "                del inputs, outputs, generated_ids\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ CUDA –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ü–∏–∫–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–π\n",
    "    # –û—á–∏—â–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –º–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä\n",
    "    del model, processor\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    logger.info(\"[inference] –ü–∞–º—è—Ç—å CUDA –æ—á–∏—â–µ–Ω–∞ –ø–æ—Å–ª–µ —Å–∂–∞—Ç–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–π, –º–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω—ã\")\n",
    "\n",
    "\n",
    "def _get_sentence_embedding(sentence: str) -> np.ndarray:\n",
    "    model, tokenizer = get_rubert_model_and_tokenizer()\n",
    "    encoded = tokenizer(\n",
    "        sentence if isinstance(sentence, str) else \"\",\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded)\n",
    "    embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "    result = embeddings[0].numpy()\n",
    "    # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –æ—Ç —Ç–µ–Ω–∑–æ—Ä–æ–≤ —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "    del encoded, outputs, embeddings\n",
    "    # –ù–µ —É–¥–∞–ª—è–µ–º model –∏ tokenizer - –æ–Ω–∏ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø–æ–≤—Ç–æ—Ä–Ω–æ\n",
    "    return result\n",
    "\n",
    "\n",
    "def _semantic_similarity_russian(text1: str, text2: str) -> float:\n",
    "    emb1 = _get_sentence_embedding(text1).reshape(1, -1)\n",
    "    emb2 = _get_sentence_embedding(text2).reshape(1, -1)\n",
    "    similarity = cosine_similarity(emb1, emb2)[0][0]\n",
    "    return float(similarity)\n",
    "\n",
    "\n",
    "def _compute_image_similarity(df: pd.DataFrame, unique_links: List[str], captions: List[str]) -> None:\n",
    "    if \"–°—Ö–æ–∂–µ—Å—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è –∫–∞—Ä—Ç–∏–Ω–∫–∏\" not in df.columns:\n",
    "        df[\"–°—Ö–æ–∂–µ—Å—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è –∫–∞—Ä—Ç–∏–Ω–∫–∏\"] = 0.0\n",
    "    link_to_caption = {link: captions[idx] for idx, link in enumerate(unique_links)}\n",
    "    total_images = len([i for i in range(len(df)) if str(df.loc[i, \"–ö–∞—Ä—Ç–∏–Ω–∫–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞\"]) != \"no image\"])\n",
    "    logger.info(f\"[inference] –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è {total_images} –∑–∞–ø–∏—Å–µ–π —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏\")\n",
    "    processed = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            link = str(df.loc[i, \"–ö–∞—Ä—Ç–∏–Ω–∫–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞\"]) if \"–ö–∞—Ä—Ç–∏–Ω–∫–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞\" in df.columns else \"no image\"\n",
    "            if link and link != \"no image\" and link in link_to_caption:\n",
    "                processed += 1\n",
    "                if processed % 50 == 0 or processed == total_images:\n",
    "                    elapsed = time.time() - start\n",
    "                    speed = processed / elapsed if elapsed > 0 else 0\n",
    "                    eta = (total_images - processed) / speed if speed > 0 else 0\n",
    "                    logger.info(f\"[inference] –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏: {processed}/{total_images} ({elapsed:.1f} —Å–µ–∫, {speed:.1f} –∑–∞–ø/—Å–µ–∫, ETA: {eta:.1f} —Å–µ–∫)\")\n",
    "                    # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –∫–∞–∂–¥—ã–µ 100 –∑–∞–ø–∏—Å–µ–π\n",
    "                    if processed % 100 == 0 and torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                        gc.collect()\n",
    "                        logger.debug(f\"[inference] –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ {processed} –∑–∞–ø–∏—Å–µ–π —Å—Ö–æ–∂–µ—Å—Ç–∏\")\n",
    "                person_text = str(df.loc[i, \"–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞\"]) if \"–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞\" in df.columns else \"\"\n",
    "                llm_text = link_to_caption[link]\n",
    "                score = _semantic_similarity_russian(person_text, llm_text)\n",
    "                df.loc[i, \"–°—Ö–æ–∂–µ—Å—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è –∫–∞—Ä—Ç–∏–Ω–∫–∏\"] = score\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ CUDA –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ü–∏–∫–ª–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ö–æ–∂–µ—Å—Ç–∏\n",
    "    # –ú–æ–¥–µ–ª—å ruBERT –≥–ª–æ–±–∞–ª—å–Ω–∞—è, –Ω–æ –æ—á–∏—â–∞–µ–º –∫–µ—à GPU\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    logger.info(\"[inference] –ü–∞–º—è—Ç—å CUDA –æ—á–∏—â–µ–Ω–∞ –ø–æ—Å–ª–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏\")\n",
    "\n",
    "\n",
    "def _build_inference_prompt(row: pd.Series) -> str:\n",
    "    question_num = int(row.get(\"‚Ññ –≤–æ–ø—Ä–æ—Å–∞\", 0))\n",
    "    question_text = str(row.get(\"–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞\", \"\"))\n",
    "    response = str(row.get(\"–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞\", \"\"))\n",
    "    test_type = \"–æ–ø–∏—Å–∞–Ω–∏–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏\" if int(row.get(\"–¢–∏–ø —Ç–µ—Å—Ç–∞\", 0)) == 1 else \"–¥–∏–∞–ª–æ–≥\"\n",
    "    max_score_map = {1: 1, 2: 2, 3: 1, 4: 2}\n",
    "    max_score = max_score_map.get(question_num, 2)\n",
    "\n",
    "    prompt = (\n",
    "        \"–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ—Ü–µ–Ω–∫–µ —É—Å—Ç–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ —ç–∫–∑–∞–º–µ–Ω–µ –ø–æ —Ä—É—Å—Å–∫–æ–º—É —è–∑—ã–∫—É –¥–ª—è –∏–Ω–æ—Å—Ç—Ä–∞–Ω—Ü–µ–≤.\\n\"\n",
    "        \"–ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏:\\n\"\n",
    "        \"1) –û—à–∏–±–∫–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤–∞—Ö –∏ –µ–¥–∏–Ω–∏—á–Ω—ã–µ –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ñ—Ä–∞–∑ –Ω–µ —Å—á–∏—Ç–∞—é—Ç—Å—è –æ—à–∏–±–∫–æ–π.\\n\"\n",
    "        \"2) –¢–µ—Å—Ç–∏—Ä—É–µ–º—ã–π –¥–æ–ª–∂–µ–Ω –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∫–æ–º–º—É–Ω–∏–∫–∞—Ç–∏–≤–Ω—É—é –∑–∞–¥–∞—á—É (–¥–æ–ª–∂–µ–Ω –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –¥–æ–±–∏—Ç—å—Å—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —Å–≤–æ–π –≤–æ–ø—Ä–æ—Å).\\n\"\n",
    "        \"3) –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Ç–µ—Å—Ç–∏—Ä—É–µ–º–æ–≥–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –ø–æ–ª–Ω—ã–º–∏.\\n\"\n",
    "        \"–û—Ü–µ–Ω–∏ –æ—Ç–≤–µ—Ç –ø–æ —Å—Ç—Ä–æ–≥–æ–π —à–∫–∞–ª–µ.\\n\\n\"\n",
    "        \"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\\n\"\n",
    "        f\"- ‚Ññ –≤–æ–ø—Ä–æ—Å–∞: {question_num}\\n\"\n",
    "        f\"- –¢–∏–ø –∑–∞–¥–∞–Ω–∏—è: {test_type}\\n\"\n",
    "    )\n",
    "\n",
    "    if int(row.get(\"–¢–∏–ø —Ç–µ—Å—Ç–∞\", 0)) == 1 and pd.notna(row.get(\"–°—Ö–æ–∂–µ—Å—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è –∫–∞—Ä—Ç–∏–Ω–∫–∏\")):\n",
    "        similarity = float(row.get(\"–°—Ö–æ–∂–µ—Å—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è –∫–∞—Ä—Ç–∏–Ω–∫–∏\"))\n",
    "        prompt += f\"- –°—Ö–æ–∂–µ—Å—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º: {similarity:.2f}\\n\"\n",
    "\n",
    "    prompt += (\n",
    "        f\"- –í–æ–ø—Ä–æ—Å: {question_text}\\n\"\n",
    "        f\"- –û—Ç–≤–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–∞: {response}\\n\\n\"\n",
    "        f\"–û—Ü–µ–Ω–∫–∞ (—Ü–µ–ª–æ–µ —á–∏—Å–ª–æ –æ—Ç 0 –¥–æ {max_score}):\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def _extract_score(text: str, question_num: int) -> int:\n",
    "    numbers = re.findall(r\"\\d+\", text)\n",
    "    if not numbers:\n",
    "        return 0\n",
    "    score = int(numbers[0])\n",
    "    max_score = {1: 1, 2: 2, 3: 1, 4: 2}.get(question_num, 2)\n",
    "    if score < 0:\n",
    "        return 0\n",
    "    if score > max_score:\n",
    "        return max_score\n",
    "    return score\n",
    "\n",
    "\n",
    "def _predict_batch(prompts: List[str], question_nums: List[int]) -> List[int]:\n",
    "    model, tokenizer = get_text_model_and_tokenizer()\n",
    "    logger.info(f\"[inference] –ó–∞–ø—É—Å–∫ –±–∞—Ç—á–µ–≤–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è {len(prompts)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "    logger.info(f\"[inference] –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è {len(prompts)} –ø—Ä–æ–º–ø—Ç–æ–≤...\")\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "    ).to(model.device)\n",
    "    logger.info(f\"[inference] –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n",
    "\n",
    "    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
    "    logger.info(f\"[inference] –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏...\")\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    logger.info(f\"[inference] –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n",
    "\n",
    "    # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "    logger.info(f\"[inference] –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...\")\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    predictions: List[int] = []\n",
    "    for i, full_text in enumerate(decoded):\n",
    "        prompt = prompts[i]\n",
    "        generated = full_text[len(prompt):].strip()\n",
    "        score = _extract_score(generated, question_nums[i])\n",
    "        predictions.append(score)\n",
    "    \n",
    "    # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –æ—Ç –≤—Å–µ—Ö –±–æ–ª—å—à–∏—Ö —Ç–µ–Ω–∑–æ—Ä–æ–≤\n",
    "    del inputs, outputs, decoded\n",
    "    # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (–æ–Ω–∏ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ, –Ω–æ –ª–æ–∫–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ —É–¥–∞–ª—è–µ–º)\n",
    "    del model, tokenizer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    logger.info(f\"[inference] –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω—ã, —Å—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {np.mean(predictions):.2f} –∑–∞ {elapsed:.1f} —Å–µ–∫ ({len(prompts)/elapsed:.1f} –ø—Ä–µ–¥—Å–∫–∞–∑/—Å–µ–∫)\")\n",
    "    logger.info(\"[inference] –ü–∞–º—è—Ç—å CUDA –æ—á–∏—â–µ–Ω–∞ –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ü–µ–Ω–æ–∫, –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω—ã\")\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def run_inference(input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame c –¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–π –∫–æ–ª–æ–Ω–∫–æ–π\n",
    "    \"–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞\" –∏ –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω—ã–º–∏ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–º–∏ –ø–æ–ª—è–º–∏.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(f\"[inference] ========== –ó–ê–ü–£–°–ö –ò–ù–§–ï–†–ï–ù–°–ê: {len(input_df)} —Å—Ç—Ä–æ–∫ ==========\")\n",
    "    df = input_df.copy()\n",
    "\n",
    "    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è NaN –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "    logger.info(\"[inference] –®–∞–≥ 1/5: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö\")\n",
    "    if \"–ö–∞—Ä—Ç–∏–Ω–∫–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞\" in df.columns:\n",
    "        df[\"–ö–∞—Ä—Ç–∏–Ω–∫–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞\"] = df[\"–ö–∞—Ä—Ç–∏–Ω–∫–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞\"].fillna(\"no image\")\n",
    "    else:\n",
    "        df[\"–ö–∞—Ä—Ç–∏–Ω–∫–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞\"] = \"no image\"\n",
    "\n",
    "    # –¢–∏–ø —Ç–µ—Å—Ç–∞: 1 –µ—Å–ª–∏ –µ—Å—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫–∞, –∏–Ω–∞—á–µ 0\n",
    "    df[\"–¢–∏–ø —Ç–µ—Å—Ç–∞\"] = 0\n",
    "    for idx in range(len(df)):\n",
    "        try:\n",
    "            link = str(df.loc[idx, \"–ö–∞—Ä—Ç–∏–Ω–∫–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞\"]) if \"–ö–∞—Ä—Ç–∏–Ω–∫–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞\" in df.columns else \"no image\"\n",
    "            df.loc[idx, \"–¢–∏–ø —Ç–µ—Å—Ç–∞\"] = 0 if (not link or link == \"no image\") else 1\n",
    "        except Exception:\n",
    "            df.loc[idx, \"–¢–∏–ø —Ç–µ—Å—Ç–∞\"] = 0\n",
    "\n",
    "    # –ß–∏—Å—Ç–∏–º HTML –≤ \"–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞\"\n",
    "    if \"–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞\" in df.columns:\n",
    "        df[\"–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞\"] = df[\"–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞\"].apply(_filter_text)\n",
    "\n",
    "    # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏\n",
    "    saved_links: List[str] = []\n",
    "    if \"–ö–∞—Ä—Ç–∏–Ω–∫–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞\" in df.columns:\n",
    "        for v in df[\"–ö–∞—Ä—Ç–∏–Ω–∫–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞\"].values:\n",
    "            if isinstance(v, str) and v != \"no image\" and v not in saved_links:\n",
    "                saved_links.append(v)\n",
    "\n",
    "    logger.info(f\"[inference] –ù–∞–π–¥–µ–Ω–æ {len(saved_links)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {int(df['–¢–∏–ø —Ç–µ—Å—Ç–∞'].sum())} —Å—Ç—Ä–æ–∫ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏\")\n",
    "\n",
    "    # –ü–æ–¥–ø–∏—Å–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º (VL)\n",
    "    logger.info(\"[inference] –®–∞–≥ 2/5: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–¥–ø–∏—Å–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º (VL)\")\n",
    "    images_text: List[str] = _caption_images(saved_links)\n",
    "\n",
    "    # –°–∂–∞—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ –¥–æ –æ–ø–∏—Å–∞–Ω–∏—è –∫–∞—Ä—Ç–∏–Ω–∫–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–∏–ø —Ç–µ—Å—Ç–∞ == 1)\n",
    "    logger.info(\"[inference] –®–∞–≥ 3/5: –°–∂–∞—Ç–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–π –¥–ª—è –∑–∞–¥–∞–Ω–∏–π —Å –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏\")\n",
    "    _summarize_transcription_for_image_tasks(df)\n",
    "\n",
    "    # –°—Ö–æ–∂–µ—Å—Ç—å –æ–ø–∏—Å–∞–Ω–∏–π\n",
    "    logger.info(\"[inference] –®–∞–≥ 4/5: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏\")\n",
    "    _compute_image_similarity(df, saved_links, images_text)\n",
    "\n",
    "    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "    logger.info(\"[inference] –®–∞–≥ 5/5: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ü–µ–Ω–æ–∫\")\n",
    "    prompts = df.apply(_build_inference_prompt, axis=1).tolist()\n",
    "    qnums = [int(v) if pd.notna(v) else 0 for v in df.get(\"‚Ññ –≤–æ–ø—Ä–æ—Å–∞\", pd.Series([0] * len(df)))]\n",
    "    predictions = _predict_batch(prompts, qnums)\n",
    "\n",
    "    df[\"–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞\"] = predictions\n",
    "    \n",
    "    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        logger.info(\"[inference] –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ CUDA –∫–µ—à–∞\")\n",
    "    gc.collect()\n",
    "    logger.info(\"[inference] –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–±–æ—Ä –º—É—Å–æ—Ä–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"[inference] ========== –ò–ù–§–ï–†–ï–ù–° –ó–ê–í–ï–†–®–ï–ù: {len(df)} —Å—Ç—Ä–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞ {elapsed:.1f} —Å–µ–∫ ==========\")\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. –ó–∞–≥—Ä—É–∑–∫–∞ CSV —Ñ–∞–π–ª–∞ –∏–∑ –ø–∞–ø–∫–∏ `data`\n",
    "\n",
    "–°–æ–∑–¥–∞–π—Ç–µ –ø–∞–ø–∫—É `data` –∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ç—É–¥–∞ –≤–∞—à CSV —Ñ–∞–π–ª —á–µ—Ä–µ–∑ Colab –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å (Files -> Upload), –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–¥ –Ω–∏–∂–µ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìÅ –ó–ê–ì–†–£–ó–ö–ê CSV –§–ê–ô–õ–ê\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É data –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç\n",
    "os.makedirs('data', exist_ok=True)\n",
    "print(\"\\nüìÇ –ü–∞–ø–∫–∞ data/ –≥–æ—Ç–æ–≤–∞\")\n",
    "\n",
    "# –í–∞—Ä–∏–∞–Ω—Ç 1: –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª –Ω–∞–ø—Ä—è–º—É—é\n",
    "print(\"\\nüì§ –ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV —Ñ–∞–π–ª:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "if uploaded:\n",
    "    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Ñ–∞–π–ª –≤ –ø–∞–ø–∫—É data\n",
    "    csv_filename = list(uploaded.keys())[0]\n",
    "    if not csv_filename.startswith('data/'):\n",
    "        import shutil\n",
    "        shutil.move(csv_filename, f'data/{csv_filename}')\n",
    "        csv_filename = f'data/{csv_filename}'\n",
    "    print(f\"\\n‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: {csv_filename}\")\n",
    "    file_size = os.path.getsize(csv_filename) / 1024 / 1024  # MB\n",
    "    print(f\"   –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size:.2f} MB\")\n",
    "else:\n",
    "    # –í–∞—Ä–∏–∞–Ω—Ç 2: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ–∞–π–ª –∏–∑ –ø–∞–ø–∫–∏ data\n",
    "    data_files = [f for f in os.listdir('data') if f.endswith('.csv')]\n",
    "    if data_files:\n",
    "        csv_filename = f\"data/{data_files[0]}\"\n",
    "        file_size = os.path.getsize(csv_filename) / 1024 / 1024  # MB\n",
    "        print(f\"\\n‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ñ–∞–π–ª –∏–∑ –ø–∞–ø–∫–∏ data: {csv_filename}\")\n",
    "        print(f\"   –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size:.2f} MB\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå –û–®–ò–ë–ö–ê: CSV —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
    "        print(\"   –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –≤ –ø–∞–ø–∫—É data/ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∑–∞–≥—Ä—É–∑–∫—É –≤—ã—à–µ\")\n",
    "        csv_filename = None\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. –ß—Ç–µ–Ω–∏–µ CSV —Ñ–∞–π–ª–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üìñ –ß–¢–ï–ù–ò–ï CSV –§–ê–ô–õ–ê\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if csv_filename and os.path.exists(csv_filename):\n",
    "    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É —Ñ–∞–π–ª–∞\n",
    "    with open(csv_filename, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        first_line = f.readline().strip()\n",
    "        print(f\"–ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ —Ñ–∞–π–ª–∞: {first_line[:100]}...\")\n",
    "    \n",
    "    # –ï—Å–ª–∏ –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç ';', –∑–Ω–∞—á–∏—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å —Ç–æ—á–Ω–æ ';'\n",
    "    if ';' in first_line:\n",
    "        sep = ';'\n",
    "        print(f\"‚úÖ –û–ø—Ä–µ–¥–µ–ª–µ–Ω —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å: '{sep}' (–ø–æ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–µ)\")\n",
    "    else:\n",
    "        # –ü—Ä–æ–±—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏\n",
    "        delimiters = [';', ',', '\\t']\n",
    "        with open(csv_filename, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            lines = [f.readline() for _ in range(3)]\n",
    "        \n",
    "        delimiter_scores = {}\n",
    "        for delim in delimiters:\n",
    "            counts = [line.count(delim) for line in lines if line.strip()]\n",
    "            if counts:\n",
    "                delimiter_scores[delim] = sum(counts) / len(counts)\n",
    "        \n",
    "        if delimiter_scores:\n",
    "            sep = max(delimiter_scores, key=delimiter_scores.get)\n",
    "            print(f\"‚úÖ –û–ø—Ä–µ–¥–µ–ª–µ–Ω —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å: '{sep}' (–∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)\")\n",
    "        else:\n",
    "            sep = ';'\n",
    "            print(f\"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: '{sep}'\")\n",
    "    \n",
    "    # –ß–∏—Ç–∞–µ–º CSV —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º\n",
    "    try:\n",
    "        df = pd.read_csv(csv_filename, sep=sep, encoding='utf-8', on_bad_lines='skip', engine='python')\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã\n",
    "        if len(df.columns) == 1 and ';' in str(df.columns[0]):\n",
    "            print(\"‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∏ –Ω–µ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ, –ø—Ä–æ–±—É–µ–º –∏—Å–ø—Ä–∞–≤–∏—Ç—å...\")\n",
    "            # –†–∞–∑–¥–µ–ª—è–µ–º –ø–µ—Ä–≤—É—é –∫–æ–ª–æ–Ω–∫—É –µ—Å–ª–∏ –æ–Ω–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ –Ω–∞–∑–≤–∞–Ω–∏—è\n",
    "            first_col = df.columns[0]\n",
    "            if ';' in str(first_col):\n",
    "                # –ß–∏—Ç–∞–µ–º –∑–∞–Ω–æ–≤–æ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º\n",
    "                df = pd.read_csv(csv_filename, sep=';', encoding='utf-8', on_bad_lines='skip', engine='python')\n",
    "        \n",
    "        print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫\")\n",
    "        print(f\"‚úÖ –ö–æ–ª–æ–Ω–æ–∫: {len(df.columns)}\")\n",
    "        print(f\"–ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}\")\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ\n",
    "        expected_cols = ['Id —ç–∫–∑–∞–º–µ–Ω–∞', 'Id –≤–æ–ø—Ä–æ—Å–∞', '‚Ññ –≤–æ–ø—Ä–æ—Å–∞', '–¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞', \n",
    "                        '–ö–∞—Ä—Ç–∏–Ω–∫–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞', '–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞', '–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞']\n",
    "        found_cols = [col for col in expected_cols if col in df.columns]\n",
    "        if len(found_cols) >= 3:\n",
    "            print(f\"‚úÖ –ù–∞–π–¥–µ–Ω—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {found_cols}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ù–µ –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –Ω–∞–π–¥–µ–Ω—ã!\")\n",
    "            print(f\"–û–∂–∏–¥–∞–ª–∏—Å—å: {expected_cols}\")\n",
    "            print(f\"–ù–∞–π–¥–µ–Ω—ã: {list(df.columns)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ CSV: {e}\")\n",
    "        # –ü—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏\n",
    "        for alt_sep in [';', ',', '\\t']:\n",
    "            if alt_sep != sep:\n",
    "                try:\n",
    "                    print(f\"–ü—Ä–æ–±—É–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å '{alt_sep}'...\")\n",
    "                    df = pd.read_csv(csv_filename, sep=alt_sep, encoding='utf-8', on_bad_lines='skip', engine='python')\n",
    "                    if len(df.columns) > 1:\n",
    "                        print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º '{alt_sep}'\")\n",
    "                        print(f\"–ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}\")\n",
    "                        sep = alt_sep\n",
    "                        break\n",
    "                except Exception as e2:\n",
    "                    continue\n",
    "        else:\n",
    "            print(f\"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å CSV —Ñ–∞–π–ª\")\n",
    "            df = None\n",
    "    \n",
    "    if df is not None:\n",
    "        if len(df) == 0:\n",
    "            print(\"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: DataFrame –ø—É—Å—Ç–æ–π!\")\n",
    "            df = None\n",
    "        elif len(df.columns) == 1:\n",
    "            print(\"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –¢–æ–ª—å–∫–æ –æ–¥–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞! –§–∞–π–ª –Ω–µ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω –ø—Ä–∞–≤–∏–ª—å–Ω–æ.\")\n",
    "            print(\"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–∫–∞–∑–∞—Ç—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –≤—Ä—É—á–Ω—É—é –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞.\")\n",
    "else:\n",
    "    print(\"‚ùå –û–®–ò–ë–ö–ê: CSV —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. –ó–∞–ø—É—Å–∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\n",
    "\n",
    "‚ö†Ô∏è **–í–ù–ò–ú–ê–ù–ò–ï**: –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ (–æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–∏–Ω—É—Ç –¥–æ —á–∞—Å–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞).\n",
    "\n",
    "**–õ–æ–≥–∏ –±—É–¥—É—Ç –≤—ã–≤–æ–¥–∏—Ç—å—Å—è –∑–¥–µ—Å—å, –≤ —ç—Ç–æ–π —è—á–µ–π–∫–µ**, –ø–æ–¥ –∫–æ–¥–æ–º. –í—ã —É–≤–∏–¥–∏—Ç–µ:\n",
    "- –ü—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥–æ–≥–æ —ç—Ç–∞–ø–∞ (1/5, 2/5, –∏ —Ç.–¥.)\n",
    "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π\n",
    "- –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è –¥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è (ETA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–ø—É—Å–∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\n",
    "if df is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üöÄ –ù–ê–ß–ê–õ–û –ò–ù–§–ï–†–ï–ù–°–ê\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"üìä –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è —Ñ–∞–π–ª: {csv_filename}\")\n",
    "    print(f\"üìà –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(df)}\")\n",
    "    print(f\"üìã –ö–æ–ª–æ–Ω–æ–∫ –≤ —Ñ–∞–π–ª–µ: {len(df.columns)}\")\n",
    "    print(\"\\n‚è≥ –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –∑–∞–ø—É—â–µ–Ω, –ª–æ–≥–∏ –±—É–¥—É—Ç –≤—ã–≤–æ–¥–∏—Ç—å—Å—è –Ω–∏–∂–µ...\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    # –ó–∞–ø—É—Å–∫–∞–µ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å - –≤—Å–µ –ª–æ–≥–∏ –±—É–¥—É—Ç –≤–∏–¥–Ω—ã –∑–¥–µ—Å—å\n",
    "    result_df = run_inference(df)\n",
    "    \n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úÖ –ò–ù–§–ï–†–ï–ù–° –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:\")\n",
    "    print(f\"   ‚Ä¢ –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(result_df)}\")\n",
    "    print(f\"   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {result_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞'].mean():.2f}\")\n",
    "    print(f\"   ‚Ä¢ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫:\")\n",
    "    score_counts = result_df['–û—Ü–µ–Ω–∫–∞ —ç–∫–∑–∞–º–µ–Ω–∞—Ç–æ—Ä–∞'].value_counts().sort_index()\n",
    "    for score, count in score_counts.items():\n",
    "        percentage = (count / len(result_df)) * 100\n",
    "        print(f\"     - –û—Ü–µ–Ω–∫–∞ {score}: {count} –∑–∞–ø–∏—Å–µ–π ({percentage:.1f}%)\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"‚ùå –û–®–ò–ë–ö–ê: DataFrame –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω!\")\n",
    "    print(\"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ CSV —Ñ–∞–π–ª –±—ã–ª –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∏ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω.\")\n",
    "    result_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
    "print(\"=\" * 80)\n",
    "print(\"üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–ê\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if result_df is not None:\n",
    "    output_filename = \"result_with_scores.csv\"\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É\n",
    "    result_df.to_csv(output_filename, index=False, sep=';', encoding='utf-8')\n",
    "    file_size = os.path.getsize(output_filename) / 1024 / 1024  # MB\n",
    "    print(f\"\\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {output_filename}\")\n",
    "    print(f\"   –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size:.2f} MB\")\n",
    "    \n",
    "    # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–ø–∫—É data\n",
    "    data_output = f\"data/{output_filename}\"\n",
    "    result_df.to_csv(data_output, index=False, sep=';', encoding='utf-8')\n",
    "    print(f\"\\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {data_output}\")\n",
    "\n",
    "    # –°–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª\n",
    "    print(f\"\\nüì• –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞...\")\n",
    "    from google.colab import files\n",
    "    files.download(output_filename)\n",
    "    print(\"‚úÖ –§–∞–π–ª –≥–æ—Ç–æ–≤ –∫ —Å–∫–∞—á–∏–≤–∞–Ω–∏—é!\")\n",
    "    print(\"\\nüéâ –í–°–ï –ì–û–¢–û–í–û! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –∏ –≥–æ—Ç–æ–≤—ã –∫ —Å–∫–∞—á–∏–≤–∞–Ω–∏—é.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå –û–®–ò–ë–ö–ê: –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è!\")\n",
    "    print(\"   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —É—Å–ø–µ—à–Ω–æ.\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
